

local-lms-for-coding.md

https://www.reddit.com/r/LocalLLaMA/comments/1h4tidn/best_local_llm_for_coding/


https://aider.chat/docs/leaderboards/

ollama/qwen2.5-coder:32b

Qwen 2.5 coder takes the cake, although it's definitely still worth to give Codestral or Mistral Large a shot and alter between models to keep things versatile.

qwen 32b coder q4km

DeepSeek-V3 or Qwen2.5-Coder-32B

Code Llama (especially the 13B version), DeepSeek-Coder, and StarCoder2 
WizardCoder and Mixtral 8x7B

Code Llama:
Built on Llama 2, it offers excellent code generation and reasoning abilities, especially the 13B version. 
DeepSeek-Coder:
Known for its strong performance, particularly with Python and general coding tasks. 
StarCoder2:
A powerful open-source model, with 7B and 15B parameter versions, offering good code generation capabilities. 
WizardCoder:
Fine-tuned on StarCoder using the Evol-Instruct method, it excels at complex instruction following and code generation. 
Mixtral 8x7B:
A powerful open-source model, known for its strong performance in coding tasks. 
Other Notable Options:
Mistral 7B: While smaller, it can outperform Llama 2 13B in some benchmarks. 
Phi-2: A lightweight and fast option for simpler coding tasks. 
Stable Code 3B: Excels at FIM (Fill-in-the-Middle) tasks, making it useful for code completion. 
GPT4All: A versatile framework for running LLMs locally, supporting various models including those for coding. 
Ollama: A popular open-source engine for running LLMs locally, known for its simplicity and efficiency. 
Factors to Consider When Choosing:
Model Size:
Larger models (like 13B and above) generally offer better performance but require more resources. 
Specific Coding Tasks:
Some models might be better suited for certain programming languages or specific types of coding tasks (e.g., code completion, code generation, reasoning). 
Hardware Limitations:
Consider the computational resources (CPU, GPU, RAM) available on your machine. 
Licensing:
Check the licensing terms for commercial use, particularly if you plan to integrate the model into a product. 
Ease of Use:
Some frameworks like GPT4All and Ollama simplify the process of running LLMs locally. 

https://pieces.app/blog/best-llm-for-coding-cloud-vs-local

https://nutstudio.imyfone.com/llm-tips/best-llm-for-coding/

https://blog.promptlayer.com/best-llms-for-coding/

https://github.com/continuedev/what-llm-to-use

# C#

https://www.reddit.com/r/dotnet/comments/1i8l9f8/what_llm_model_would_you_recommend_for_a_locally/

deepseek coder is good https://deepseekcoder.github.io/

And v2 here

https://github.com/deepseek-ai/DeepSeek-Coder-V2

Qwen 2.5 coder

eepSeek based on a recommendation from u/souley76 above, so far it seems promising. No python non-sequiturs. I'll keep Phi in 

CodeBooga-34B-v0.1 is even better than code llama and wizard coder.

https://huggingface.co/oobabooga/CodeBooga-34B-v0.1

brew install llama.cpp

CodeBooga-34B-v0.1: 22
WizardCoder-Python-34B-V1.0: 12
Phind-CodeLlama-34B-v2: 7
CodeBooga-Reversed-34B-v0.1: 1







