# vLLM

vllm.md

*   fast and easy-to-use library for LLM inference and serving.

*   https://docs.vllm.ai/en/latest/index.html

*   https://github.com/vllm-project/vllm

```
pip install vllm
```
