Context Window


context-window.md

A "context window" in the realm of large language models (LLMs) refers to the amount of text, measured in tokens, that the model can consider or "remember" when generating or understanding language. Essentially, it's the model's working memory. A larger context window allows the model to process longer inputs and incorporate more information, leading to potentially more coherent and contextually relevant responses. 

https://www.ibm.com/think/topics/context-window

Key points about context windows:
Tokens:
The context window is defined by the number of tokens the model can process. Tokens are the smallest building blocks of text, like words or parts of words. 
Working Memory Analogy:
Think of the context window as the LLM's working memory, determining how much information it can hold while generating a response. 
Impact on LLM Performance:
A larger context window can lead to increased accuracy, fewer "hallucinations," more coherent responses, longer conversations, and the ability to analyze longer sequences of data. 
Trade-offs:
While larger context windows offer benefits, they also come with increased computational costs and potential vulnerabilities to adversarial attacks. 
Examples:
Different LLMs have different context window sizes. For example, Llama 2 has a 4,000 token context window, while Claude 2 can handle up to 100,000 tokens

https://www.techtarget.com/whatis/definition/context-window

https://docs.anthropic.com/en/docs/build-with-claude/context-windows

https://blog.google/technology/ai/long-context-window-ai-models/

https://www.youtube.com/watch?v=-QVoIxEpFkM

https://www.youtube.com/watch?v=TeQDr4DkLYo


## Videos

*   What is a Context Window? Unlocking LLM Secrets

    *   https://youtu.be/-QVoIxEpFkM



